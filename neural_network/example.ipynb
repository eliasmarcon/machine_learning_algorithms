{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Perceptron "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(self) -> None:\n",
    "        \n",
    "        \"\"\"\n",
    "        This function generates artificial data for the exercise.\n",
    "        \"\"\"\n",
    "\n",
    "        self.X = np.empty(100)\n",
    "        self.y = np.empty(100)\n",
    "        \n",
    "        for i in range(100):\n",
    "            if random.random() < 0.5:\n",
    "                self.X[i] = np.random.normal(loc=-1.25, scale=0.75)\n",
    "                self.y[i] = 0\n",
    "            else:\n",
    "                self.X[i] = np.random.normal(loc=1.25, scale=0.75)\n",
    "                self.y[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.7</td>\n",
       "      <td>48.9</td>\n",
       "      <td>75.0</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57.5</td>\n",
       "      <td>32.8</td>\n",
       "      <td>23.5</td>\n",
       "      <td>11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>120.2</td>\n",
       "      <td>19.6</td>\n",
       "      <td>11.6</td>\n",
       "      <td>13.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>199.8</td>\n",
       "      <td>2.6</td>\n",
       "      <td>21.2</td>\n",
       "      <td>15.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>66.1</td>\n",
       "      <td>5.8</td>\n",
       "      <td>24.2</td>\n",
       "      <td>12.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>214.7</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23.8</td>\n",
       "      <td>35.1</td>\n",
       "      <td>65.9</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>97.5</td>\n",
       "      <td>7.6</td>\n",
       "      <td>7.2</td>\n",
       "      <td>13.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>204.1</td>\n",
       "      <td>32.9</td>\n",
       "      <td>46.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>195.4</td>\n",
       "      <td>47.7</td>\n",
       "      <td>52.9</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>67.8</td>\n",
       "      <td>36.6</td>\n",
       "      <td>114.0</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>281.4</td>\n",
       "      <td>39.6</td>\n",
       "      <td>55.8</td>\n",
       "      <td>24.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>69.2</td>\n",
       "      <td>20.5</td>\n",
       "      <td>18.3</td>\n",
       "      <td>11.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>147.3</td>\n",
       "      <td>23.9</td>\n",
       "      <td>19.1</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TV  Radio  Newspaper  Sales\n",
       "0   230.1   37.8       69.2   22.1\n",
       "1    44.5   39.3       45.1   10.4\n",
       "2    17.2   45.9       69.3   12.0\n",
       "3   151.5   41.3       58.5   16.5\n",
       "4   180.8   10.8       58.4   17.9\n",
       "5     8.7   48.9       75.0    7.2\n",
       "6    57.5   32.8       23.5   11.8\n",
       "7   120.2   19.6       11.6   13.2\n",
       "8     8.6    2.1        1.0    4.8\n",
       "9   199.8    2.6       21.2   15.6\n",
       "10   66.1    5.8       24.2   12.6\n",
       "11  214.7   24.0        4.0   17.4\n",
       "12   23.8   35.1       65.9    9.2\n",
       "13   97.5    7.6        7.2   13.7\n",
       "14  204.1   32.9       46.0   19.0\n",
       "15  195.4   47.7       52.9   22.4\n",
       "16   67.8   36.6      114.0   12.5\n",
       "17  281.4   39.6       55.8   24.4\n",
       "18   69.2   20.5       18.3   11.3\n",
       "19  147.3   23.9       19.1   14.6"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/advertising.csv\")\n",
    "df = df[:20]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# from perceptron import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('Sales', axis=1), df['Sales'], test_size=0.2, random_state=42)\n",
    "\n",
    "# # Standardize the data\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "print(type(X_train))\n",
    "print(type(y_train))\n",
    "print(type(X_test))\n",
    "print(type(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, bias : int = 1, loss_function : str = None, activation_function : str = None):\n",
    "        \n",
    "        self.__bias = bias\n",
    "        self.__loss_function = loss_function\n",
    "        self.__activation_function = activation_function\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        # Initialize weights with small random values\n",
    "        self.__weights = np.random.randn(X_train.shape[1]) / np.sqrt(X_train.shape[1])\n",
    "        \n",
    "        self.X_train = X_train.to_numpy()\n",
    "        self.y_train = y_train.to_numpy()\n",
    "        \n",
    "        self.__get_ml_type()\n",
    "        \n",
    "    def train(self, epochs : int = 100, learning_rate : int = 0.01):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = self.forward(self.X_train)\n",
    "    \n",
    "            # Compute and print loss\n",
    "            loss = self.loss(y_pred, self.y_train)\n",
    "\n",
    "            # Backward pass\n",
    "            self.backward(y_pred, self.y_train)\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                # print(f\"Y_Pred: {y_pred}\")\n",
    "                print(f'Epoch: {epoch}, Loss: {loss}\\n')\n",
    "                \n",
    "    \n",
    "    ############################################################################################################\n",
    "    ####################################### Forward and Backward Pass ##########################################\n",
    "    ############################################################################################################\n",
    "    def forward(self, X):\n",
    "        \n",
    "        return self.activation_function(np.dot(X, self.__weights) + self.__bias)\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \n",
    "        if self.__ml_type == \"binary\":\n",
    "            self.backward_classification(y_pred, y_true)\n",
    "        else:\n",
    "            self.backward_regression(y_pred, y_true)\n",
    "\n",
    "    def backward_regression(self, y_pred, y_true):\n",
    "        \n",
    "        dL_dw = 0\n",
    "        dL_db = 0\n",
    "        \n",
    "        if self.__loss_function == \"mse\":\n",
    "            \n",
    "            # MSE derivative is (y_pred - y_true)\n",
    "            # hyperbolic tangent derivative is 1 - tanh^2(x)\n",
    "            dL_dw = (y_pred - y_true) * ( 1 - y_pred**2) * self.X_train\n",
    "            dL_db = (y_pred - y_true) * ( 1 - y_pred**2)\n",
    "        \n",
    "        elif self.__loss_function == \"mae\":\n",
    "            \n",
    "            # MAE derivative is sign(y_pred - y_true)\n",
    "            # hyperbolic tangent derivative is 1 - tanh^2(x)\n",
    "            dL_dw = np.sign(y_pred - y_true) * ( 1 - y_pred**2) * self.X_train\n",
    "            dL_db = np.sign(y_pred - y_true) * ( 1 - y_pred**2)\n",
    "            \n",
    "        self.__weights -= self.learning_rate * dL_dw\n",
    "        self.__bias -= self.learning_rate * dL_db\n",
    "        \n",
    "    def backward_classification(self, y_pred, y_true):\n",
    "        \n",
    "        # derivative of the cross entropy loss function - ( y_true / y_pred ) - ( ( 1 - y_true ) / ( 1 - y_pred ) )\n",
    "        # derivative of the sigmoid function sigmoid(x) * ( 1 - sigmoid(x) )\n",
    "        dL_dw = - ( ( y_true / y_pred ) - ( ( 1 - y_true ) / ( 1 - y_pred ) ) ) * y_pred * ( 1 - y_pred ) * self.X_train\n",
    "        dL_db = - ( ( y_true / y_pred ) - ( ( 1 - y_true ) / ( 1 - y_pred ) ) ) * y_pred * ( 1 - y_pred )\n",
    "        \n",
    "        self.__weights -= self.learning_rate * dL_dw\n",
    "        self.__bias -= self.learning_rate * dL_db    \n",
    "        \n",
    "\n",
    "    ############################################################################################################\n",
    "    ####################################### Activation Functions ###############################################\n",
    "    ############################################################################################################\n",
    "    def activation_function(self, x):\n",
    "        \n",
    "        return self.get_actvation_function(x)\n",
    "    \n",
    "    def get_actvation_function(self, x):\n",
    "        \n",
    "        if (self.__activation_function is None and self.__ml_type == \"binary\") or (self.__activation_function == \"sigmoid\" and self.__ml_type == \"binary\"):\n",
    "            return self.sigmoid(x)\n",
    "        \n",
    "        elif (self.__activation_function is None and self.__ml_type == \"regression\") or (self.__activation_function == \"tanh\" and self.__ml_type == \"regression\"):\n",
    "            return self.tan_h(x)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        # Clip the values of y_pred to avoid division by zero\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def tan_h(self, x):\n",
    "        return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "        \n",
    "    ############################################################################################################\n",
    "    ####################################### Loss Functions #####################################################\n",
    "    ############################################################################################################   \n",
    "    def loss(self, y_pred, y_true):\n",
    "        \n",
    "        return self.get_loss_function(y_pred, y_true)\n",
    "    \n",
    "    def get_loss_function(self, y_pred, y_true):\n",
    "        \n",
    "        if (self.__loss_function is None and self.__ml_type == \"binary\") or (self.__loss_function == \"binary_crossentropy\" and self.__ml_type == \"binary\"):\n",
    "            return self.__binary_crossentropy(y_pred, y_true)\n",
    "        elif (self.__loss_function is None and self.__ml_type == \"regression\") or (self.__loss_function == \"mse\" and self.__ml_type == \"regression\"):\n",
    "            return self.__mse(y_pred, y_true)\n",
    "        elif self.__loss_function == \"mae\" and self.__ml_type == \"regression\":\n",
    "            return self.__mae(y_pred, y_true)\n",
    "        \n",
    "    def __binary_crossentropy(self, y_pred, y_true):\n",
    "        epsilon = 1e-7\n",
    "        losses = - (y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
    "        return np.mean(losses)\n",
    "    \n",
    "    def __mse(self, y_pred, y_true):\n",
    "        return np.mean( ( y_pred - y_true ) ** 2 )\n",
    "    \n",
    "    def __mae(self, y_pred, y_true):\n",
    "        return np.mean( np.abs( y_pred - y_true ) )\n",
    "                      \n",
    "    ############################################################################################################\n",
    "    ####################################### Helper Functions ###################################################\n",
    "    ############################################################################################################\n",
    "    def __get_ml_type(self):\n",
    "        \n",
    "        if np.unique(self.y_train).shape[0] == 2:\n",
    "            self.__ml_type = \"binary\"\n",
    "        else:\n",
    "            self.__ml_type = \"regression\"\n",
    "            \n",
    "    \n",
    "    ### Backward Pass MSE and Sigmoid function\n",
    "    # Compute the derivative of the mse loss function\n",
    "    # # dL_dw = sum(map(lambda xi, y_pred_i, y_true_i : ( y_true_i - ( 1 / 1 + np.exp(-y_pred_i) ) * ( ( xi * np.exp(y_pred_i) ) /  ( ( 1 + np.exp(-y_pred_i) ) **2 ) ) ), self.X_train, y_pred, y_true))\n",
    "    # dL_dw = np.sum( y_true - ( 1 / 1 + np.exp(-y_pred) ) * ( ( self.X_train.T * np.exp(-y_pred) ) / ( ( 1 + np.exp(-y_pred) ) **2 ) ) , axis = 1)\n",
    "    \n",
    "    # # dL_db = sum(map(lambda y_pred_i, y_true_i : ( y_true_i - ( 1 / 1 + np.exp(-y_pred_i) ) * ( ( np.exp(-y_pred_i) ) / ( ( 1 + np.exp(-y_pred_i) ) **2 ) ) ), y_pred, y_true))\n",
    "    # dL_db = np.sum( y_true - ( 1 / 1 + np.exp(-y_pred) ) * ( ( np.exp(-y_pred) ) / ( ( 1 + np.exp(-y_pred) ) **2 ) ) )\n",
    "    \n",
    "    # self.__weights -= self.learning_rate * dL_dw\n",
    "    # self.__bias -= self.learning_rate * dL_db\n",
    "\n",
    "    # # self.__weights = self.__weights + self.learning_rate * np.dot((y_true - y_pred), self.X_train)\n",
    "    # # self.__bias = self.__bias + self.learning_rate * np.sum(y_true - y_pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Perceptron:\n",
    "#     def __init__(self, bias=1):\n",
    "#         self.__bias = bias\n",
    "#         self.__weights = None\n",
    "#         self.learning_rate = None  # Initialize learning_rate\n",
    "\n",
    "#     def fit(self, X_train, y_train):\n",
    "#         # Initialize weights with small random values\n",
    "#         self.__weights = np.random.randn(X_train.shape[1]) / np.sqrt(X_train.shape[1])\n",
    "        \n",
    "\n",
    "#         self.X_train = X_train\n",
    "#         self.y_train = y_train.to_numpy()\n",
    "\n",
    "#     def sigmoid(self, x):\n",
    "#         # Clip values to avoid overflow in np.exp\n",
    "#         return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "#     def forward(self, X):\n",
    "#         return self.sigmoid(np.dot(X, self.__weights) + self.__bias)\n",
    "\n",
    "#     def loss(self, y_pred, y_true):\n",
    "#         epsilon = 1e-7\n",
    "#         loss = - (y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
    "#         return np.mean(loss)\n",
    "\n",
    "#     def backward(self, y_pred, y_true):\n",
    "        \n",
    "#         epsilon = 1e-7\n",
    "#         # d_cross_entropy = sum(map(lambda y_pred_i, y_true_i : (y_pred_i - y_true_i) / (y_pred_i * (1 - y_pred_i) + epsilon), y_pred, y_true))\n",
    "#         d_cross_entropy = np.sum((y_pred - y_true) / (y_pred * (1 - y_pred) + epsilon))\n",
    "        \n",
    "#         # d_weights = sum(map(lambda xi, y_pred_i : ( xi * np.exp(-y_pred_i) ) / ( ( 1 + np.exp(-y_pred_i) ) ** 2 ), self.X_train, y_pred))\n",
    "#         d_weights = np.sum(self.X_train * np.exp(-y_pred.reshape(-1, 1)) / ((1 + np.exp(-y_pred.reshape(-1, 1))) ** 2), axis=0)\n",
    "        \n",
    "#         # d_bias = sum(map(lambda y_pred_i : ( np.exp(-y_pred_i) ) / ( ( 1 + np.exp(-y_pred_i) ) ** 2 ), y_pred))\n",
    "#         d_bias = np.sum(np.exp(-y_pred) / ((1 + np.exp(-y_pred)) ** 2))\n",
    "        \n",
    "#         # print(f\"d_cross_entropy: {d_cross_entropy}\\n\")\n",
    "        \n",
    "#         dL_dw = d_cross_entropy * d_weights\n",
    "#         dL_db = d_cross_entropy * d_bias\n",
    "        \n",
    "#         # Update weights and bias\n",
    "#         self.__weights -= self.learning_rate * dL_dw\n",
    "#         self.__bias -= self.learning_rate * dL_db\n",
    "        \n",
    "\n",
    "#     def train(self, epochs=100, learning_rate=0.01):\n",
    "#         self.learning_rate = learning_rate\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "            \n",
    "#             # Forward pass\n",
    "#             y_pred = self.forward(self.X_train)\n",
    "\n",
    "#             # Compute and print loss\n",
    "#             loss = self.loss(y_pred, self.y_train)\n",
    "\n",
    "#             # print(f\"Weights: {self.__weights}\\nBias: {self.__bias}\\nLoss: {loss}\\n\")\n",
    "\n",
    "#             # Backward pass\n",
    "#             self.backward(y_pred, self.y_train)\n",
    "\n",
    "#             if epoch % 100 == 0:\n",
    "#                 # print(f\"Y_Pred: {y_pred}\")\n",
    "#                 print(f'Epoch: {epoch}, Loss: {loss}\\n')\n",
    "                \n",
    "#         # training accuracy\n",
    "#         y_pred = self.forward(self.X_train)\n",
    "        \n",
    "#         y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "#         print(f\"Training Accuracy: {np.mean(y_pred == self.y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 212.18298920628365\n",
      "\n",
      "Epoch: 10, Loss: 212.18298920628365\n",
      "\n",
      "Epoch: 20, Loss: 212.18298920628365\n",
      "\n",
      "Epoch: 30, Loss: 212.18298920628365\n",
      "\n",
      "Epoch: 40, Loss: 212.18298920628365\n",
      "\n",
      "Epoch: 50, Loss: 212.18298920628365\n",
      "\n",
      "Epoch: 60, Loss: 212.18298920628365\n",
      "\n",
      "Epoch: 70, Loss: 212.18298920628365\n",
      "\n",
      "Epoch: 80, Loss: 212.18298920628365\n",
      "\n",
      "Epoch: 90, Loss: 212.18298920628365\n",
      "\n"
     ]
    }
   ],
   "source": [
    "perceptron = Perceptron()\n",
    "perceptron.fit(X_train, y_train)\n",
    "perceptron.train(epochs = 100, learning_rate = 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
